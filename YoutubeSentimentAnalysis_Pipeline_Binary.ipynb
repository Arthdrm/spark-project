{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aBc-Po01BP2l"
      },
      "source": [
        "# 0) Initial Preparation\n",
        "- Installing packages\n",
        "- Importing packages\n",
        "- Spark session initialization\n",
        "- Database connection"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "biELh7lu4Roc"
      },
      "source": [
        "**Installing Packages**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i37UT_lo_uYz"
      },
      "outputs": [],
      "source": [
        "# Install Spark NLP\n",
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
        "# Install contraction module\n",
        "!pip install contractions\n",
        "# Install the transformers module from HuggingFace\n",
        "!pip install -q transformers\n",
        "# Install the explode library\n",
        "!pip install explode"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GGhHm-ay4YD2"
      },
      "source": [
        "**Importing Packages**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WrjYHALbzlI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import googleapiclient.discovery\n",
        "import re\n",
        "import sparknlp\n",
        "import contractions\n",
        "import explode\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import *\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import *\n",
        "from pyspark.ml.evaluation import *\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
        "from pyspark.ml.util import DefaultParamsWritable, DefaultParamsReadable\n",
        "\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from transformers import pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UEkvqfaR4hCR"
      },
      "source": [
        "**Spark Session Instantiation (GPU accelerated session)**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AwVE8BpoBVVH",
        "outputId": "38545de1-c3ee-4b72-cdb6-6463a4292c9c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.2.3'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark = SparkSession.builder\\\n",
        "    .appName(\"Spark NLP\")\\\n",
        "    .master(\"local[*]\")\\\n",
        "    .config(\"spark.driver.memory\",\"12G\")\\\n",
        "    .config(\"spark.mongodb.read.connection.uri\", \"Your mongodb cluster URL\")\\\n",
        "    .config(\"spark.mongodb.write.connection.uri\", \"Your mongodb cluster URL\")\\\n",
        "    .config(\"spark.mongodb.write.maxBatchSize\", 4096)\\\n",
        "    .config(\"spark.mongodb.write.ordered\", False)\\\n",
        "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.1.1,com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:4.4.3\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br2sM5yt98-N",
        "outputId": "605be08e-75ec-460c-9dc0-ba74a2c77ae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gJezu5XXEbAV"
      },
      "source": [
        "# 1) Data Extraction : Youtube Comment Scrapping\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SjYSaUC24rau"
      },
      "source": [
        "**Youtube Comments Scrapping Function**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMKYB4WCtuZy"
      },
      "outputs": [],
      "source": [
        "#------------- Youtube comments scrapping function -------------#\n",
        "def get_comment(counter, video_id):\n",
        "    # Bypassing the https verification.\n",
        "    # *DO NOT* leave this option enabled in production.\n",
        "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
        "\n",
        "    # Specifiying the API name we want to use from Google.\n",
        "    api_service_name = \"youtube\"\n",
        "    # Specifying the API version.\n",
        "    api_version = \"v3\"\n",
        "    # Specifying the API key.\n",
        "    DEVELOPER_KEY = \"Insert Your Developer Key\"\n",
        "\n",
        "    # Building the initial part of the https request.\n",
        "    # To build this part of the request : https://www.googleapis.com/youtube/v3\n",
        "    youtube = googleapiclient.discovery.build(\n",
        "        api_service_name, api_version, developerKey = DEVELOPER_KEY)\n",
        "\n",
        "    # Specifying which resource & method to be used for the request.\n",
        "    request = youtube.commentThreads().list(\n",
        "        videoId=video_id,\n",
        "        part=\"snippet\",\n",
        "        maxResults=100,\n",
        "        order=\"relevance\"\n",
        "    )\n",
        "\n",
        "    # Variable to store all the top-level comments of a particular video\n",
        "    response_all = []\n",
        "\n",
        "    # Sending the initial request\n",
        "    response = request.execute()\n",
        "    # Getting all the comment from the initial response (100 comments)\n",
        "    for i in range(len(response[\"items\"])):\n",
        "      response_all.append({\n",
        "          \"_id\" : counter[0],\n",
        "          \"videoId\" : video_id,\n",
        "          \"userId\" : response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorChannelId\"][\"value\"],\n",
        "          \"text\" : response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textOriginal\"],\n",
        "          \"date\" : response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"],\n",
        "      })\n",
        "      counter[0] = counter[0] + 1\n",
        "\n",
        "    # Loop to retrieve the subsequent top-level comments\n",
        "    while True:\n",
        "      if \"nextPageToken\" in response.keys():\n",
        "          pg_token = response[\"nextPageToken\"]\n",
        "          request = youtube.commentThreads().list(\n",
        "              videoId=video_id,\n",
        "              part=\"snippet\",\n",
        "              pageToken=pg_token,\n",
        "              maxResults=100,\n",
        "              order=\"relevance\"\n",
        "          )\n",
        "          response = request.execute()\n",
        "          for i in range(len(response[\"items\"])):\n",
        "            response_all.append({\n",
        "                \"_id\" : counter[0],\n",
        "                \"videoId\" : video_id,\n",
        "                \"userId\" : response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorChannelId\"][\"value\"],\n",
        "                \"text\" : response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textOriginal\"],\n",
        "                \"date\" : response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"],\n",
        "            })\n",
        "            counter[0] = counter[0] + 1\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    return response_all"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o-1wW-Pm4ywq"
      },
      "source": [
        "**Scrapping Youtube Comments**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfFfO_TDeeCp"
      },
      "outputs": [],
      "source": [
        "# Comments retrieval Iphone-14\n",
        "counter = [0]\n",
        "comment = []\n",
        "comment = get_comment(counter, \"SdLShOCvVeM\")\n",
        "comment = comment + get_comment(counter, \"KLPZzf-wwlE\")\n",
        "comment = comment + get_comment(counter, \"qDMY_n5b348\")\n",
        "comment = comment + get_comment(counter, \"pTCgWVjB6UE\")\n",
        "comment = comment + get_comment(counter, \"-E0iNG6uTxk\")\n",
        "comment = comment + get_comment(counter, \"3NjpX5TBajY\")\n",
        "comment = comment + get_comment(counter, \"P3Q5XYacz5E\")\n",
        "comment = comment + get_comment(counter, \"NCe4WpoA03Q\")\n",
        "comment = comment + get_comment(counter, \"oTtbIf1TfL8\")\n",
        "comment = comment + get_comment(counter, \"VcgE32zVPLw\")\n",
        "\n",
        "# Printing how many comments are succesfully retrieved\n",
        "print(counter[0])\n",
        "df = spark.createDataFrame(comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmaVd41BdSGi"
      },
      "outputs": [],
      "source": [
        "# Comments retrieval Iphone-15\n",
        "counter = [0]\n",
        "comment = []\n",
        "comment = get_comment(counter, \"tzHLhlBMp6o\")\n",
        "comment = comment + get_comment(counter, \"O3y-MdGwhrk\")\n",
        "comment = comment + get_comment(counter, \"Sfs1uX5coyI\")\n",
        "comment = comment + get_comment(counter, \"VIe7MPPwCaA\")\n",
        "comment = comment + get_comment(counter, \"vchqdJdLB3w\")\n",
        "comment = comment + get_comment(counter, \"yUWZsuFQwiE\")\n",
        "comment = comment + get_comment(counter, \"BTJR-tZ_hmM\")\n",
        "\n",
        "# Printing how many comments are succesfully retrieved\n",
        "print(counter[0])\n",
        "df = spark.createDataFrame(comment)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2oNW11yKkpLj"
      },
      "source": [
        "**⬆️ Writing The Dataframe To a MongoDB Collection [WRITE]**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTVwxLltkJf5"
      },
      "outputs": [],
      "source": [
        "# Iphone 14 Pro dataset\n",
        "df.write.format(\"mongodb\")\\\n",
        "        .mode(\"append\")\\\n",
        "        .option(\"database\",\"db_analisis_sentimen\")\\\n",
        "        .option(\"collection\", \"comment_iphone14_raw\")\\\n",
        "        .save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdlZOC3mkJf5"
      },
      "outputs": [],
      "source": [
        "# Iphone 15 dataset\n",
        "df.write.format(\"mongodb\")\\\n",
        "        .mode(\"append\")\\\n",
        "        .option(\"database\",\"db_analisis_sentimen\")\\\n",
        "        .option(\"collection\", \"comment_iphone15_raw\")\\\n",
        "        .save()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sfKUPTqH4XXU"
      },
      "source": [
        "# 2) Data Preprocessing\n",
        "All operations performed :\n",
        "- Case folding (lowercase)\n",
        "- Links removal\n",
        "- Duplicate comments removal\n",
        "- Contraction handling\n",
        "- Special character & number removal\n",
        "- Document assembler (For spark NLP)\n",
        "- Language detection & filtering (only english)\n",
        "- Tokenizing\n",
        "- Spell checking\n",
        "- Stop words removal\n",
        "- Lemmatization\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "385uRXbBk9Dk"
      },
      "source": [
        "**⬇️ Loading the collection into a dataframe [READ]**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJnTa23jjvIN"
      },
      "outputs": [],
      "source": [
        "# Iphone 14 dataset\n",
        "df = spark.read.format(\"mongodb\")\\\n",
        "        .option(\"database\",\"db_analisis_sentimen\")\\\n",
        "        .option(\"collection\", \"comment_iphone14_raw\")\\\n",
        "        .load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkL2oSdWcAT4"
      },
      "outputs": [],
      "source": [
        "# Iphone 15 dataset\n",
        "df = spark.read.format(\"mongodb\")\\\n",
        "        .option(\"database\",\"db_analisis_sentimen\")\\\n",
        "        .option(\"collection\", \"comment_iphone15_raw\")\\\n",
        "        .load()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gKNfAOFVVG2z"
      },
      "source": [
        "**Custom Transformers Definition**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN1_TqxES0cm"
      },
      "outputs": [],
      "source": [
        "class TextLower(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
        "  @keyword_only\n",
        "  def __init__(self, inputCol=None, outputCol=None):\n",
        "      super().__init__()\n",
        "      kwargs = self._input_kwargs\n",
        "      self.setParams(**kwargs)\n",
        "\n",
        "  @keyword_only\n",
        "  def setParams(self, inputCol=None, outputCol=None):\n",
        "      kwargs = self._input_kwargs\n",
        "      return self._set(**kwargs)\n",
        "\n",
        "  def setInputCol(self, new_inputCol):\n",
        "      return self.setParams(inputCol=new_inputCol)\n",
        "  def setOutputCol(self, new_outputCol):\n",
        "      return self.setParams(outputCol=new_outputCol)\n",
        "\n",
        "  def _transform(self, df):\n",
        "    if not self.isSet(\"inputCol\"):\n",
        "        raise ValueError(\"No input col is defined!\")\n",
        "\n",
        "    input_col = self.getInputCol()\n",
        "    output_col = self.getOutputCol()\n",
        "\n",
        "    return df.withColumn(output_col, lower(df[input_col]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N446OytQFC6u"
      },
      "outputs": [],
      "source": [
        "class RemoveLinks(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
        "  @keyword_only\n",
        "  def __init__(self, inputCol=None, outputCol=None):\n",
        "      super().__init__()\n",
        "      kwargs = self._input_kwargs\n",
        "      self.setParams(**kwargs)\n",
        "\n",
        "  @keyword_only\n",
        "  def setParams(self, inputCol=None, outputCol=None):\n",
        "      kwargs = self._input_kwargs\n",
        "      return self._set(**kwargs)\n",
        "\n",
        "  def setInputCol(self, new_inputCol):\n",
        "      return self.setParams(inputCol=new_inputCol)\n",
        "\n",
        "  def setOutputCol(self, new_outputCol):\n",
        "      return self.setParams(outputCol=new_outputCol)\n",
        "\n",
        "  def _transform(self, df):\n",
        "    if not self.isSet(\"inputCol\"):\n",
        "        raise ValueError(\"No input col is defined!\")\n",
        "\n",
        "    input_col = self.getInputCol()\n",
        "    output_col = self.getOutputCol()\n",
        "\n",
        "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "\n",
        "    return df.filter(regexp_extract(df[input_col], url_pattern, 0) == '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YivnCjAnsMeQ"
      },
      "outputs": [],
      "source": [
        "class RemoveDuplicates(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
        "  @keyword_only\n",
        "  def __init__(self, inputCol=None, outputCol=None):\n",
        "      super().__init__()\n",
        "      kwargs = self._input_kwargs\n",
        "      self.setParams(**kwargs)\n",
        "\n",
        "  @keyword_only\n",
        "  def setParams(self, inputCol=None, outputCol=None):\n",
        "      kwargs = self._input_kwargs\n",
        "      return self._set(**kwargs)\n",
        "\n",
        "  def setInputCol(self, new_inputCol):\n",
        "      return self.setParams(inputCol=new_inputCol)\n",
        "\n",
        "  def setOutputCol(self, new_outputCol):\n",
        "      return self.setParams(outputCol=new_outputCol)\n",
        "\n",
        "  def _transform(self, df):\n",
        "    if not self.isSet(\"inputCol\"):\n",
        "        raise ValueError(\"No input col is defined!\")\n",
        "\n",
        "    input_col = self.getInputCol()\n",
        "    output_col = self.getOutputCol()\n",
        "\n",
        "    return df.dropDuplicates([input_col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hi8SCCkPscY1"
      },
      "outputs": [],
      "source": [
        "class RemoveDuplicates(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
        "  @keyword_only\n",
        "  def __init__(self, inputCol=None, outputCol=None):\n",
        "      super().__init__()\n",
        "      kwargs = self._input_kwargs\n",
        "      self.setParams(**kwargs)\n",
        "\n",
        "  @keyword_only\n",
        "  def setParams(self, inputCol=None, outputCol=None):\n",
        "      kwargs = self._input_kwargs\n",
        "      return self._set(**kwargs)\n",
        "\n",
        "  def setInputCol(self, new_inputCol):\n",
        "      return self.setParams(inputCol=new_inputCol)\n",
        "\n",
        "  def setOutputCol(self, new_outputCol):\n",
        "      return self.setParams(outputCol=new_outputCol)\n",
        "\n",
        "  def _transform(self, df):\n",
        "    if not self.isSet(\"inputCol\"):\n",
        "        raise ValueError(\"No input col is defined!\")\n",
        "\n",
        "    input_col = self.getInputCol()\n",
        "    output_col = self.getOutputCol()\n",
        "\n",
        "    return df.dropDuplicates([input_col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlpoEiiC-bik"
      },
      "outputs": [],
      "source": [
        "class SymbolRemoval(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
        "  @keyword_only\n",
        "  def __init__(self, inputCol=None, outputCol=None):\n",
        "      super().__init__()\n",
        "      kwargs = self._input_kwargs\n",
        "      self.setParams(**kwargs)\n",
        "\n",
        "  @keyword_only\n",
        "  def setParams(self, inputCol=None, outputCol=None):\n",
        "      kwargs = self._input_kwargs\n",
        "      return self._set(**kwargs)\n",
        "\n",
        "  def setInputCol(self, new_inputCol):\n",
        "      return self.setParams(inputCol=new_inputCol)\n",
        "\n",
        "  def setOutputCol(self, new_outputCol):\n",
        "      return self.setParams(outputCol=new_outputCol)\n",
        "\n",
        "  def _transform(self, df):\n",
        "    if not self.isSet(\"inputCol\"):\n",
        "        raise ValueError(\"No input col is defined!\")\n",
        "\n",
        "    input_col = self.getInputCol()\n",
        "    output_col = self.getOutputCol()\n",
        "\n",
        "    df = df.withColumn(output_col, regexp_replace(input_col, '[^a-z\\s]', '')) \\\n",
        "              .withColumn(output_col, regexp_replace(input_col, '[:;=]-?[\\)D\\(\\[\\]pP\\{\\}]|<3|[\\*\\']?[:;=8][\\-o\\*\\']?[\\)D\\]\\[\\\\\\/\\|\\(\\)pP3\\{\\}]', '')) \\\n",
        "              .withColumn(output_col, regexp_replace(input_col, '\\s+|\\n', ' '))\n",
        "\n",
        "    return df.filter(~((df[output_col] == '') | (df[output_col] == ' ')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsb6q14xGUfL"
      },
      "outputs": [],
      "source": [
        "class FilterEN(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
        "  @keyword_only\n",
        "  def __init__(self, inputCol=None, outputCol=None):\n",
        "      super().__init__()\n",
        "      kwargs = self._input_kwargs\n",
        "      self.setParams(**kwargs)\n",
        "\n",
        "  @keyword_only\n",
        "  def setParams(self, inputCol=None, outputCol=None):\n",
        "      kwargs = self._input_kwargs\n",
        "      return self._set(**kwargs)\n",
        "\n",
        "  def setInputCol(self, new_inputCol):\n",
        "      return self.setParams(inputCol=new_inputCol)\n",
        "\n",
        "  def setOutputCol(self, new_outputCol):\n",
        "      return self.setParams(outputCol=new_outputCol)\n",
        "\n",
        "  def _transform(self, df):\n",
        "    if not self.isSet(\"inputCol\"):\n",
        "        raise ValueError(\"No input col is defined!\")\n",
        "\n",
        "    input_col = self.getInputCol()\n",
        "    output_col = self.getOutputCol()\n",
        "\n",
        "    return df.filter(df[output_col][\"result\"][0].contains('en'))\\\n",
        "            .drop(df[output_col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iFKBgvhImgm"
      },
      "outputs": [],
      "source": [
        "class FinalCleansing(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
        "  @keyword_only\n",
        "  def __init__(self, inputCol=None, outputCol=None):\n",
        "      super().__init__()\n",
        "      kwargs = self._input_kwargs\n",
        "      self.setParams(**kwargs)\n",
        "\n",
        "  @keyword_only\n",
        "  def setParams(self, inputCol=None, outputCol=None):\n",
        "      kwargs = self._input_kwargs\n",
        "      return self._set(**kwargs)\n",
        "\n",
        "  def setInputCol(self, new_inputCol):\n",
        "      return self.setParams(inputCol=new_inputCol)\n",
        "\n",
        "  def setOutputCol(self, new_outputCol):\n",
        "      return self.setParams(outputCol=new_outputCol)\n",
        "\n",
        "  def _transform(self, df):\n",
        "    if not self.isSet(\"inputCol\"):\n",
        "        raise ValueError(\"No input col is defined!\")\n",
        "\n",
        "    input_col = self.getInputCol()\n",
        "    output_col = self.getOutputCol()\n",
        "\n",
        "    df = df.drop(df.document)\\\n",
        "            .drop(df.token)\\\n",
        "            .drop(df.stop_word_clean)\\\n",
        "            .drop(df.spell_checked)\\\n",
        "            .withColumn(\"lemma_new\", df.lemma.result)\\\n",
        "            .drop(df.lemma)\\\n",
        "            .withColumnRenamed(\"lemma_new\", \"lemma\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5imJFBw4ltCT"
      },
      "source": [
        "**Transformers Insantiation**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYKHNSnHzFw5"
      },
      "outputs": [],
      "source": [
        "# User-defined transformers\n",
        "textLower = TextLower(inputCol=\"text\", outputCol=\"text\")\n",
        "removeLink = RemoveLinks(inputCol=\"text\", outputCol=\"text\")\n",
        "removeDuplicates = RemoveDuplicates(inputCol=\"text\", outputCol=\"text\")\n",
        "contractionHandling = ContractionHandling(inputCol=\"text\", outputCol=\"text\")\n",
        "symbolRemoval = SymbolRemoval(inputCol=\"text\", outputCol=\"text\")\n",
        "filterEN = FilterEN(inputCol=\"language\", outputCol=\"language\")\n",
        "finalCleansing = FinalCleansing(inputCol=\"text\", outputCol=\"text\")\n",
        "\n",
        "# Pretrained transformers (annotator) from sparkNLP\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "                    .setInputCol(\"text\")\\\n",
        "                    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "            .setInputCols(\"document\") \\\n",
        "            .setOutputCol(\"token\")\n",
        "\n",
        "spellChecker = NorvigSweetingModel.pretrained() \\\n",
        "            .setInputCols(\"token\") \\\n",
        "            .setOutputCol(\"spell_checked\")\n",
        "\n",
        "stopWordsCleaner = StopWordsCleaner.pretrained()\\\n",
        "                  .setInputCols(\"spell_checked\")\\\n",
        "                  .setOutputCol(\"stop_word_clean\")\\\n",
        "                  .setCaseSensitive(False)\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained() \\\n",
        "            .setInputCols(\"stop_word_clean\") \\\n",
        "            .setOutputCol(\"lemma\")\n",
        "\n",
        "languageDetection = LanguageDetectorDL.pretrained(\"ld_wiki_tatoeba_cnn_375\", \"xx\")\\\n",
        "                    .setInputCols(\"document\")\\\n",
        "                    .setOutputCol(\"language\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6yIQ4l_Pl0OM"
      },
      "source": [
        "**Pipeline Insantiation & Implementation**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsFLAQaMzJhb"
      },
      "outputs": [],
      "source": [
        "# Pipeline object instatiation\n",
        "pipeline = Pipeline() \\\n",
        "    .setStages([\n",
        "      textLower,\n",
        "      removeLink,\n",
        "      removeDuplicates,\n",
        "      contractionHandling,\n",
        "      symbolRemoval,\n",
        "      documentAssembler,\n",
        "      languageDetection,\n",
        "      filterEN,\n",
        "      tokenizer,\n",
        "      spellChecker,\n",
        "      stopWordsCleaner,\n",
        "      lemmatizer,\n",
        "      finalCleansing\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW0w3TfTcKU7"
      },
      "outputs": [],
      "source": [
        "# Using the pipeline\n",
        "df_lemma = pipeline.fit(df).transform(df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "td37SGRQKsmp"
      },
      "source": [
        "**⬆️ Writing the dataframe to a MongoDB collection [WRITE]**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmnfz_Rej32B"
      },
      "outputs": [],
      "source": [
        "# Iphone 14 dataset\n",
        "df_lemma.write.format(\"mongodb\")\\\n",
        "        .mode(\"append\")\\\n",
        "        .option(\"database\",\"db_analisis_sentimen\")\\\n",
        "        .option(\"collection\", \"comment_iphone15_cleaned\")\\\n",
        "        .save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jASAudUb0JIP"
      },
      "outputs": [],
      "source": [
        "# Iphone 15 dataset\n",
        "df_lemma.write.format(\"mongodb\")\\\n",
        "        .mode(\"append\")\\\n",
        "        .option(\"database\",\"db_analisis_sentimen\")\\\n",
        "        .option(\"collection\", \"comment_iphone15_cleaned\")\\\n",
        "        .save()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kFinDvaU4cYN"
      },
      "source": [
        "# 3) Initial Data Labeling With HuggingFace (Only for iPhone 14 dataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3aVMEyMOgMth"
      },
      "source": [
        "**⬇️ Loading a collection into a dataframe [READ]**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QCMyGfsohjO"
      },
      "outputs": [],
      "source": [
        "# Iphone 14 dataset\n",
        "df = spark.read.format(\"mongodb\")\\\n",
        "        .option(\"database\",\"db_analisis_sentimen\")\\\n",
        "        .option(\"collection\", \"comment_iphone14_cleaned\")\\\n",
        "        .load()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RwxZj5YAg0l7"
      },
      "source": [
        "**Labeling Process**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXqffOO90iY7"
      },
      "outputs": [],
      "source": [
        "# Downloading a pretrained sentiment analysis model from huggingface\n",
        "model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8-PQtMjQsRj"
      },
      "outputs": [],
      "source": [
        "# Instantiating the pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path, max_length=512, truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxHbWUTAwQTM"
      },
      "outputs": [],
      "source": [
        "# Labeling the data with the pretrained pipeline\n",
        "labels = []\n",
        "for item in df_lemma.collect():\n",
        "  result = sentiment_pipeline(item.text)\n",
        "  if result[0][\"label\"] == \"negative\" :\n",
        "    labels.append([item._id, 0.0])\n",
        "  elif result[0][\"label\"] == \"positive\":\n",
        "    labels.append([item._id, 1.0])\n",
        "  else:\n",
        "    labels.append([item._id, 2.0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3CSyK2e9mTiz"
      },
      "source": [
        "**Appending The Labels To The Existing Dataframe**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtyF-WJIzL1B"
      },
      "outputs": [],
      "source": [
        "# Combining the label dataframe with the dataset dataframe\n",
        "df_label = spark.createDataFrame(labels, schema=[\"_id\", \"label\"])\n",
        "df_labeled = df_lemma.join(df_label, on=\"_id\", how=\"inner\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c846e6GZ3Zh7"
      },
      "outputs": [],
      "source": [
        "# Throwing out the comments which has a neutral sentiment\n",
        "df_labeled = df_labeled.filter((df_labeled.label == 0.0) | (df_labeled.label == 1.0))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e_9QrzCnh89t"
      },
      "source": [
        "**⬆️ Writing the dataframe to a MongoDB collection [WRITE]**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_isr682njuf"
      },
      "outputs": [],
      "source": [
        "# Iphone 14 dataset\n",
        "df_labeled.write.format(\"mongodb\")\\\n",
        "        .mode(\"append\")\\\n",
        "        .option(\"database\",\"db_analisis_sentimen\")\\\n",
        "        .option(\"collection\", \"comment_iphone14_labeled\")\\\n",
        "        .save()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SWickqU18otk"
      },
      "source": [
        "# 4) Exploratory Data Analysis & Data Experiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_oURTQvDzOz"
      },
      "outputs": [],
      "source": [
        "# Count How Many Unique Words\n",
        "df_counter = df_labeled.select(explode('lemma').alias('word'))\n",
        "df_counter_2 = df_counter.groupBy('word').count()\n",
        "print(df_counter_2.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhVNWW-rCePJ"
      },
      "outputs": [],
      "source": [
        "# Counting amounts of comments of each label\n",
        "print(\"Negative comments : \", df_labeled.filter(col('label') == 0.0).count())\n",
        "print(\"Positive comments : \", df_labeled.filter(col('label') == 1.0).count())\n",
        "print(\"Neutral comments : \", df_labeled.filter(col('label') == 2.0).count())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6_x-XGi54e-G"
      },
      "source": [
        "#5) Data Preparation (Feature Extraction & Data Splitting)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KzGxTcXLhBl5"
      },
      "source": [
        "**⬇️ Loading the collection into a dataframe [READ]**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1I04pQlN2O7B"
      },
      "outputs": [],
      "source": [
        "# Iphone 14 dataset\n",
        "df_prepare = spark.read.format(\"mongodb\")\\\n",
        "        .option(\"database\",\"db_analisis_sentimen\")\\\n",
        "        .option(\"collection\", \"comment_iphone14_labeled\")\\\n",
        "        .load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1VKhuzv6iNE"
      },
      "outputs": [],
      "source": [
        "# Iphone 15 dataset\n",
        "df_prepare = spark.read.format(\"mongodb\")\\\n",
        "        .option(\"database\",\"db_analisis_sentimen\")\\\n",
        "        .option(\"collection\", \"comment_iphone15_cleaned\")\\\n",
        "        .load()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sSMiv42ehC6C"
      },
      "source": [
        "**Feature Extraction : HashingTF**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_02g_cZUgBN"
      },
      "outputs": [],
      "source": [
        "# Evaluating TF score of each term in the comments through HashingTF.\n",
        "hashingTF = HashingTF(inputCol=\"lemma\", outputCol=\"rawFeatures\", numFeatures=8192)\n",
        "df_tf = hashingTF.transform(df_prepare)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tk7RTGtX613x"
      },
      "source": [
        "**Feature Extraction : IDF**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB_Ua04OLvjA"
      },
      "outputs": [],
      "source": [
        "# Normalizing TF score of each term in the comment through IDF.\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idf_model = idf.fit(df_tf)\n",
        "df_idf = idf_model.transform(df_tf)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KG_HPTR-ho77"
      },
      "source": [
        "**Splitting The Dataset Into Training and Testing Dataset (80/20 Split)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbgGn5z_MdWh"
      },
      "outputs": [],
      "source": [
        "# Data splitting with 80/20 ratio\n",
        "(df_training, df_testing) = df_idf.randomSplit([0.8, 0.2], seed=2023)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HHsAWZ4CUMG7"
      },
      "source": [
        "# 6) Model Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "po2XY2_bbkFu"
      },
      "source": [
        "**Multinomial Naive Bayes Model**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFrG2BQzUgss"
      },
      "outputs": [],
      "source": [
        "# Insantiating the Multinomial Naive Bayes model (estimator) from Spark ML\n",
        "mnb_estimator = NaiveBayes(modelType=\"multinomial\", featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Training the model with the training dataset\n",
        "mnb_model = mnb_estimator.fit(df_training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY7Y1wmUMxzy"
      },
      "outputs": [],
      "source": [
        "# Testing the model with the testing dataset\n",
        "df_prediction = mnb_model.transform(df_testing)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nW-m_9yh4h0o"
      },
      "source": [
        "#7) Model Evaluation (Automatic)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GsE8GTNuEQSD"
      },
      "source": [
        "**Multi-Class Evaluator**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwUAmXS-elVO",
        "outputId": "9d568e25-d824-4386-867d-b6dbbd35df5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy :  0.8415545590433483\n"
          ]
        }
      ],
      "source": [
        "# Using the multi-class evaluator to evaluate the model accuracy\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(df_prediction)\n",
        "print(\"Model accuracy : \", accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OFQbR1SRbzZ5"
      },
      "source": [
        "**Binary Evaluator**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka_Hcv6EKblp",
        "outputId": "fbbf95b2-17c7-44db-f488-c23d5931ef16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC score :  0.8400790221014941\n"
          ]
        }
      ],
      "source": [
        "# Using the binary evaluator to evaluate the model accuracy (through AUC score)\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "auc_score = evaluator.evaluate(df_prediction)\n",
        "print(\"AUC score : \", auc_score)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Aot42AVOqNlf"
      },
      "source": [
        "**Saving the model**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU72gVjStdjf"
      },
      "outputs": [],
      "source": [
        "# Saving the trained model to google drive\n",
        "mnb_model.write().overwrite().save(\"/content/drive/MyDrive/Model\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cccNIjuXpi0m"
      },
      "source": [
        "# 8) Predicting iPhone 15 dataset label with the trained multi-nomial naive bayes model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O6R_0Pm3qIWC"
      },
      "source": [
        "**Loading the model**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49BDlOH4vB4m"
      },
      "outputs": [],
      "source": [
        "mnb_model = NaiveBayesModel.load(\"/content/drive/MyDrive/Model/Model\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j2zhMeJFq4co"
      },
      "source": [
        "**Label Prediction**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9A_uRs9rllF"
      },
      "outputs": [],
      "source": [
        "# Using the model to predict the sentiment of iPhone 15 dataset\n",
        "df_prediction = mnb_model.transform(df_idf)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gsoIcdwVtrZR"
      },
      "source": [
        "**Tidying Up**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7MBqAAYtv9a"
      },
      "outputs": [],
      "source": [
        "# Removing unnecesary column from dataframe\n",
        "df_prediction = df_prediction.drop(col(\"features\"))\\\n",
        "                .drop(col(\"probability\"))\\\n",
        "                .drop(col(\"rawPrediction\"))\\\n",
        "                .drop(col(\"rawFeatures\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yp8xP5T7sYX1"
      },
      "source": [
        "**Saving the labeled dataset**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiFaCMVasgpY"
      },
      "outputs": [],
      "source": [
        "# Saving to a mongodb collection\n",
        "df_prediction.write.format(\"mongodb\")\\\n",
        "        .mode(\"append\")\\\n",
        "        .option(\"database\",\"db_analisis_sentimen\")\\\n",
        "        .option(\"collection\", \"comment_iphone15_labeled\")\\\n",
        "        .save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMO_9_dk-f4T"
      },
      "outputs": [],
      "source": [
        "# Saving to a csv file (saved at a drive directory)\n",
        "# Only saving the id, date, and prediction columns\n",
        "df_prediction.select('_id', 'date', 'prediction')\\\n",
        "              .sort(col('_id'), ascending = True)\\\n",
        "              .write.format(\"csv\")\\\n",
        "              .option(\"header\", \"true\")\\\n",
        "              .save(\"/content/drive/MyDrive/Dataset/comment_iphone15_labeled.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kFinDvaU4cYN",
        "SWickqU18otk",
        "6_x-XGi54e-G",
        "HHsAWZ4CUMG7",
        "nW-m_9yh4h0o"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
